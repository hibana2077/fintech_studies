{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.10.11 (tags/v3.10.11:7d4cc5a, Apr  5 2023, 00:38:17) [MSC v.1929 64 bit (AMD64)]\n",
      "TensorFlow version: 2.12.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import joblib\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `train_test_split` 是一個用於將數據集分割為訓練集和測試集的函數。通常在機器學習模型訓練前會使用。\n",
    "- `StandardScaler` 是一種數據預處理技術，用於將特徵標準化為平均值為0，標準差為1的數據集，這也被稱為z-score標準化。\n",
    "- `confusion_matrix` 是一種特定格式的表格，用於視覺化一個分類模型的效能。矩陣的每一列代表實際類別，每一行代表預測類別。\n",
    "- `accuracy_score` 函數用於計算分類模型的準確度，即模型正確預測的樣本數與總樣本數的比例。\n",
    "- `precision_score` 函數用於計算分類模型的精度，即模型正確預測的正樣本數與模型預測為正的總樣本數的比例。\n",
    "- `recall_score` 函數用於計算分類模型的召回率，即模型正確預測的正樣本數與實際正樣本數的比例。\n",
    "- `f1_score` 函數用於計算分類模型的F1分數，這是精度和召回率的調和平均值。\n",
    "- `ROC Curve, AUC, and ROC_AUC_Score` 這些函數用於計算和繪製ROC（Receiver Operating Characteristic）曲線，以及計算AUC（Area Under the Curve）值。ROC曲線是用於評估分類模型在所有可能的分類閾值下的效能，AUC值則是ROC曲線下的面積，用於衡量模型的整體性能。\n",
    "- `classification_report` 函數生成一個包含主要分類指標（如精度、召回率、F1分數等）的報告。\n",
    "- `precision_recall_curve` 函數用於計算精度-召回率曲線。這是一種工具，用於視覺化在不同閾值下模型的精度和召回率之間的權衡。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "DATA_LOC = 'MEXC_ETHUSDTP1m_iso.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>RSI</th>\n",
       "      <th>bullish</th>\n",
       "      <th>bearish</th>\n",
       "      <th>neutral</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2023-06-22T07:36:00+08:00</th>\n",
       "      <td>1896.75</td>\n",
       "      <td>1897.23</td>\n",
       "      <td>1896.74</td>\n",
       "      <td>1897.00</td>\n",
       "      <td>57.717497</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-06-22T07:35:00+08:00</th>\n",
       "      <td>1895.85</td>\n",
       "      <td>1896.75</td>\n",
       "      <td>1895.80</td>\n",
       "      <td>1896.74</td>\n",
       "      <td>56.783005</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-06-22T07:34:00+08:00</th>\n",
       "      <td>1895.80</td>\n",
       "      <td>1895.86</td>\n",
       "      <td>1895.30</td>\n",
       "      <td>1895.86</td>\n",
       "      <td>53.557043</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-06-22T07:33:00+08:00</th>\n",
       "      <td>1896.00</td>\n",
       "      <td>1896.00</td>\n",
       "      <td>1895.80</td>\n",
       "      <td>1895.81</td>\n",
       "      <td>53.373414</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-06-22T07:32:00+08:00</th>\n",
       "      <td>1896.32</td>\n",
       "      <td>1896.32</td>\n",
       "      <td>1895.99</td>\n",
       "      <td>1895.99</td>\n",
       "      <td>54.088309</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              open     high      low    close        RSI  \\\n",
       "time                                                                       \n",
       "2023-06-22T07:36:00+08:00  1896.75  1897.23  1896.74  1897.00  57.717497   \n",
       "2023-06-22T07:35:00+08:00  1895.85  1896.75  1895.80  1896.74  56.783005   \n",
       "2023-06-22T07:34:00+08:00  1895.80  1895.86  1895.30  1895.86  53.557043   \n",
       "2023-06-22T07:33:00+08:00  1896.00  1896.00  1895.80  1895.81  53.373414   \n",
       "2023-06-22T07:32:00+08:00  1896.32  1896.32  1895.99  1895.99  54.088309   \n",
       "\n",
       "                           bullish  bearish  neutral  \n",
       "time                                                  \n",
       "2023-06-22T07:36:00+08:00        0        0        1  \n",
       "2023-06-22T07:35:00+08:00        0        0        1  \n",
       "2023-06-22T07:34:00+08:00        0        0        1  \n",
       "2023-06-22T07:33:00+08:00        0        0        1  \n",
       "2023-06-22T07:32:00+08:00        0        0        1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load data\n",
    "df = pd.read_csv(DATA_LOC)\n",
    "#drop RSI = NaN\n",
    "df.dropna(subset=['RSI'], inplace=True)\n",
    "#NaN -> 0\n",
    "df.fillna(0, inplace=True)\n",
    "#reverse order\n",
    "df = df.iloc[::-1]\n",
    "#set index\n",
    "df.set_index('time', inplace=True)\n",
    "#create bullish/bearish labels\n",
    "df['bullish'] = (df[\"Regular Bullish\"] > 0) | (df[\"Hidden Bullish\"] > 0) | (df[\"Regular Bullish Label\"] > 0) | (df[\"Hidden Bullish Label\"] > 0)\n",
    "df['bearish'] = (df[\"Regular Bearish\"] > 0) | (df[\"Hidden Bearish\"] > 0) | (df[\"Regular Bearish Label\"] > 0) | (df[\"Hidden Bearish Label\"] > 0)\n",
    "df['neutral'] = ~(df['bullish'] | df['bearish'])\n",
    "#drop columns\n",
    "df.drop(columns=['Regular Bearish', 'Hidden Bearish', 'Regular Bullish Label', 'Hidden Bullish Label', 'Regular Bearish Label', 'Hidden Bearish Label',\"Regular Bullish\",\"Hidden Bullish\"], inplace=True)\n",
    "#change bool to int\n",
    "df['bullish'] = df['bullish'].astype(int)\n",
    "df['bearish'] = df['bearish'].astype(int)\n",
    "df['neutral'] = df['neutral'].astype(int)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (24434, 8)\n",
      "========================================\n",
      "Data columns: Index(['open', 'high', 'low', 'close', 'RSI', 'bullish', 'bearish', 'neutral'], dtype='object')\n",
      "========================================\n",
      "Data types: open       float64\n",
      "high       float64\n",
      "low        float64\n",
      "close      float64\n",
      "RSI        float64\n",
      "bullish      int32\n",
      "bearish      int32\n",
      "neutral      int32\n",
      "dtype: object\n",
      "========================================\n",
      "==============Ground Truth==============\n",
      "Bullish count: 1695\n",
      "Bearish count: 1715\n",
      "Neutral count: 21024\n",
      "Bullish percent: 0.0694\n",
      "Bearish percent: 0.0702\n",
      "Neutral percent: 0.8604\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "#data shape\n",
    "print(f\"Data shape: {df.shape}\")\n",
    "print(\"{:=<40}\".format(\"\"))\n",
    "print(f\"Data columns: {df.columns}\")\n",
    "print(\"{:=<40}\".format(\"\"))\n",
    "print(f\"Data types: {df.dtypes}\")\n",
    "print(\"{:=<40}\".format(\"\"))\n",
    "#count bullish/bearish\n",
    "print(\"{:=^40}\".format(\"Ground Truth\"))\n",
    "print(f\"Bullish count: {df['bullish'].sum()}\")\n",
    "print(f\"Bearish count: {df['bearish'].sum()}\")\n",
    "print(f\"Neutral count: {df['neutral'].sum()}\")\n",
    "#print percent bullish/bearish/neutral\n",
    "print(f\"Bullish percent: {df['bullish'].sum()/df.shape[0]:.4f}\")\n",
    "print(f\"Bearish percent: {df['bearish'].sum()/df.shape[0]:.4f}\")\n",
    "print(f\"Neutral percent: {df['neutral'].sum()/df.shape[0]:.4f}\")\n",
    "print(\"{:=^40}\".format(\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 標準化\n",
    "\n",
    "- Z-Score標準化：這種方法將數據轉換為均值為0，標準差為1的數據集。在Python中，可以使用`sklearn.preprocessing.StandardScaler`來實現。\n",
    "\n",
    "- 最小最大標準化：這種方法將數據轉換到一個指定的範圍內，例如[0,1]。這對於神經網絡來說非常有用，因為它們通常喜歡在固定範圍內工作。在Python中，可以使用`sklearn.preprocessing.MinMaxScaler`來實現。\n",
    "\n",
    "- Log轉換：對於高度偏態的數據，我們可以對其進行log轉換以使其更接近正態分布。這在處理股價和其他金融數據時非常有用，因為它們的分佈往往有很大的尾部。可以使用`np.log`函數來實現。\n",
    "\n",
    "- 差分化（Differencing）：在時間序列分析中，我們有時會使用差分來消除時間序列數據中的趨勢和季節性成分。這通常適用於股價數據，因為我們更關注價格變化而不是實際價格。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data\n",
    "X = df.drop(columns=['bullish', 'bearish', 'neutral'])\n",
    "y = df[['bullish', 'bearish', 'neutral']]\n",
    "#scale data\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "X = pd.DataFrame(X, columns=df.columns[:-3])\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================Shape==================\n",
      "X shape: (24420, 15, 5)\n",
      "y shape: (24420, 3)\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def create_sequences(data, time_steps):\n",
    "    X = []\n",
    "    for i in range(len(data) - time_steps + 1):\n",
    "        X.append(data[i: (i + time_steps)].values)\n",
    "    return np.array(X)\n",
    "def create_y_sequences(data, time_steps):\n",
    "    Y = []\n",
    "    for i in range(len(data) - time_steps + 1):\n",
    "        Y.append(data.iloc[i + time_steps - 1].values)\n",
    "    return np.array(Y)\n",
    "\n",
    "\n",
    "# 將 data 轉換為帶有時間步長的序列\n",
    "X = create_sequences(X, 55)\n",
    "y = create_y_sequences(y, 55)\n",
    "\n",
    "print(\"{:=^40}\".format(\"Shape\"))\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")\n",
    "print(\"{:=^40}\".format(\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型建立"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 15, 128)           68608     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 15, 128)           0         \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 15, 256)           394240    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 15, 256)           0         \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 15, 512)           1574912   \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 15, 512)           0         \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 15, 256)           787456    \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 15, 256)           0         \n",
      "                                                                 \n",
      " lstm_4 (LSTM)               (None, 128)               197120    \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 3)                 387       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,022,723\n",
      "Trainable params: 3,022,723\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#model\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.LSTM(128, input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=True),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.LSTM(256, return_sequences=True),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.LSTM(512, return_sequences=True),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.LSTM(256, return_sequences=True),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.LSTM(128, return_sequences=False),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      " 41/489 [=>............................] - ETA: 1:21 - loss: 3.3346 - accuracy: 0.3491"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m#train\u001b[39;00m\n\u001b[0;32m      2\u001b[0m class_weights \u001b[39m=\u001b[39m {\u001b[39m0\u001b[39m: y\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m/\u001b[39m y[:, \u001b[39m0\u001b[39m]\u001b[39m.\u001b[39msum(), \u001b[39m1\u001b[39m: y\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m/\u001b[39m y[:, \u001b[39m1\u001b[39m]\u001b[39m.\u001b[39msum(), \u001b[39m2\u001b[39m: y\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m/\u001b[39m y[:, \u001b[39m2\u001b[39m]\u001b[39m.\u001b[39msum()}\n\u001b[1;32m----> 3\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(X_train, y_train, epochs\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m32\u001b[39;49m, validation_split\u001b[39m=\u001b[39;49m\u001b[39m0.2\u001b[39;49m, class_weight\u001b[39m=\u001b[39;49mclass_weights)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training.py:1685\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1677\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1678\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1679\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1682\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   1683\u001b[0m ):\n\u001b[0;32m   1684\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1685\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1686\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1687\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:894\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    891\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    893\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 894\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    896\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    897\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:926\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    923\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    924\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    925\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 926\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_no_variable_creation_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    927\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    928\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    929\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    930\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:143\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    141\u001b[0m   (concrete_function,\n\u001b[0;32m    142\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m--> 143\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m    144\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1757\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1753\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1754\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1755\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1756\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1757\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1758\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1759\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1760\u001b[0m     args,\n\u001b[0;32m   1761\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1762\u001b[0m     executing_eagerly)\n\u001b[0;32m   1763\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:381\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    379\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    380\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 381\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    382\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    383\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    384\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    385\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    386\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    387\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    388\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    389\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    390\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    393\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    394\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\eager\\execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 52\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     55\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#train\n",
    "class_weights = {0: y.shape[0] / y[:, 0].sum(), 1: y.shape[0] / y[:, 1].sum(), 2: y.shape[0] / y[:, 2].sum()}\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.2, class_weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=history.epoch, y=history.history['loss'], name='loss'))\n",
    "fig.add_trace(go.Scatter(x=history.epoch, y=history.history['val_loss'], name='val_loss'))\n",
    "fig.update_layout(title='Loss', xaxis_title='Epoch', yaxis_title='Loss')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot - confusion matrix\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "y_test = np.argmax(y_test, axis=1)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm = pd.DataFrame(cm, index=['bullish', 'bearish', 'neutral'], columns=['bullish', 'bearish', 'neutral'])\n",
    "fig = px.imshow(cm, color_continuous_scale='Blues')\n",
    "fig.update_layout(title='Confusion Matrix', xaxis_title='Predicted Class', yaxis_title='Actual Class')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot - candlestick(Ground Truth)\n",
    "df = pd.read_csv(DATA_LOC)\n",
    "df.fillna(0, inplace=True)\n",
    "df.set_index('time', inplace=True)\n",
    "df['bullish'] = (df[\"Regular Bullish\"] > 0) | (df[\"Hidden Bullish\"] > 0) | (df[\"Regular Bullish Label\"] > 0) | (df[\"Hidden Bullish Label\"] > 0)\n",
    "df['bearish'] = (df[\"Regular Bearish\"] > 0) | (df[\"Hidden Bearish\"] > 0) | (df[\"Regular Bearish Label\"] > 0) | (df[\"Hidden Bearish Label\"] > 0)\n",
    "df['neutral'] = ~(df['bullish'] | df['bearish'])\n",
    "df.drop(columns=['Regular Bearish', 'Hidden Bearish', 'Regular Bullish Label', 'Hidden Bullish Label', 'Regular Bearish Label', 'Hidden Bearish Label',\"Regular Bullish\",\"Hidden Bullish\"], inplace=True)\n",
    "df['bullish'] = df['bullish'].astype(int)\n",
    "df['bearish'] = df['bearish'].astype(int)\n",
    "df['neutral'] = df['neutral'].astype(int)\n",
    "df = df[['open', 'high', 'low', 'close', 'bullish', 'bearish', 'neutral']]\n",
    "# df.head()\n",
    "fig = go.Figure(data=[go.Candlestick(x=df.index, open=df['open'], high=df['high'], low=df['low'], close=df['close'])])\n",
    "fig.update_layout(title='Candlestick', xaxis_title='Time', yaxis_title='Price')\n",
    "#plot bullish/bearish/neutral\n",
    "bullish = df[df['bullish'] == 1]\n",
    "bearish = df[df['bearish'] == 1]\n",
    "fig.add_trace(go.Scatter(x=bullish.index, y=bullish['close'], mode='markers', name='Bullish', marker=dict(color='green')))\n",
    "fig.add_trace(go.Scatter(x=bearish.index, y=bearish['close'], mode='markers', name='Bearish', marker=dict(color='red')))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot - candlestick(Predict)\n",
    "df = pd.read_csv(DATA_LOC)\n",
    "df.fillna(0, inplace=True)\n",
    "df.set_index('time', inplace=True)\n",
    "df['bullish'] = (df[\"Regular Bullish\"] > 0) | (df[\"Hidden Bullish\"] > 0) | (df[\"Regular Bullish Label\"] > 0) | (df[\"Hidden Bullish Label\"] > 0)\n",
    "df['bearish'] = (df[\"Regular Bearish\"] > 0) | (df[\"Hidden Bearish\"] > 0) | (df[\"Regular Bearish Label\"] > 0) | (df[\"Hidden Bearish Label\"] > 0)\n",
    "df['neutral'] = ~(df['bullish'] | df['bearish'])\n",
    "df.drop(columns=['Regular Bearish', 'Hidden Bearish', 'Regular Bullish Label', 'Hidden Bullish Label', 'Regular Bearish Label', 'Hidden Bearish Label',\"Regular Bullish\",\"Hidden Bullish\"], inplace=True)\n",
    "df['bullish'] = df['bullish'].astype(int)\n",
    "df['bearish'] = df['bearish'].astype(int)\n",
    "df['neutral'] = df['neutral'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluation\n",
    "print(\"{:=^40}\".format(\"Evaluation\"))\n",
    "print(classification_report(y_test, y_pred, target_names=['bullish', 'bearish', 'neutral']))\n",
    "print(\"{:=^40}\".format(\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 評估指標\n",
    "\n",
    "- **精確度（Precision）**：被預測為正樣本並且預測正確的樣本數占被預測為正樣本的樣本數的比例。這是一個評價模型準確性的指標。\n",
    "\n",
    "- **召回率（Recall）**：也叫真正率（True Positive Rate, TPR），被預測為正樣本並且預測正確的樣本數占所有真實正樣本的比例。這是一個評價模型覆蓋率的指標。\n",
    "\n",
    "- **F1 分數**：精確度和召回率的調和平均數，這是一個綜合考慮了精確度和召回率的指標。如果你同時關心精確度和召回率，那麼 F1 分數是一個很好的評價指標。\n",
    "\n",
    "- **準確率（Accuracy）**：預測正確的樣本數占總樣本數的比例。這是一個評價模型整體性能的指標。\n",
    "\n",
    "- **支持度（Support）**：實際的每個類別的樣本數。\n",
    "\n",
    "- **宏平均（Macro Avg）**：先對每個類別分別計算指標，然後取平均。這種方式不考慮類別不平衡。\n",
    "\n",
    "- **加權平均（Weighted Avg）**：先對每個類別分別計算指標，然後按照每個類別的樣本數加權平均。這種方式考慮了類別不平衡。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model\n",
    "model.save('model_1m_ETH.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 優化建議\n",
    "\n",
    "- 重新採樣：對訓練數據進行重新采樣，使各類別的樣本數量更為均衡。這可以通過過採樣（增加少數類別的樣本數量），或者欠採樣（減少多數類別的樣本數量）來實現。\n",
    "\n",
    "- 使用合成樣本：例如，`SMOTE (Synthetic Minority Over-sampling Technique)` 方法可以生成少數類別的合成樣本，從而提升模型在這些類別上的預測性能。\n",
    "\n",
    "- 調整類別權重：在模型訓練過程中，增大少數類別的權重，以便讓模型在誤分類這些類別時，承受更大的損失。\n",
    "\n",
    "- 嘗試不同的模型：某些模型可能對不平衡數據更為`Robust`，例如，集成方法(陳冠霖的專題)或深度學習模型。\n",
    "\n",
    "- 嘗試其他評價指標：比如 `ROC AUC`、`PR AUC`，這些指標更關注模型在各類別之間的區分度，而不僅僅是簡單的分類精度。\n",
    "\n",
    "- 改變問題設定：如果可能的話，可以考慮將問題轉化為二分類或者排序問題。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
